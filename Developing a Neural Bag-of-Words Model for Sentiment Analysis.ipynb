{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Neural Bag-of-Words Model for Sentiment Analysis\n",
    "* The evaluation of movie review text is a classification problem often called **sentiment analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "* Separation of data into training and test sets.\n",
    "* Loading and cleaning the data to remove punctuation and numbers.\n",
    "* Defining a vocabulary of preferred words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train-test split 90-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading and Cleaning Reviews\n",
    "* Split tokens on white space\n",
    "* Remove all punctuation from words\n",
    "* Remove all words that are not purely comprised of alphabetical characters.\n",
    "* Remove all words that are known stop words.\n",
    "* Remove all words that have length <=1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory,vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path,vocab)\n",
    "        \n",
    "# save list to file\n",
    "def save_list(lines,filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "process_docs('txt_sentoken/pos',vocab)\n",
    "process_docs('txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "# keep tokens with a min occurence\n",
    "min_occurence = 2\n",
    "tokens = [k for k,c in vocab.items() if c >=min_occurence]\n",
    "print(len(tokens))\n",
    "save_list(tokens,'vocab1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag-of-Words Representation\n",
    "* A bag-of-words model is a way of extracting features from text so the text input can be used with machine learning algorithms like neural networks \n",
    "* Each document, in this case a review, is converted into a vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Converting Reviews to lines of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    text = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory,vocab):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path,vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab)\n",
    "    docs = neg + pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tokens' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-279bba1c8a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# load all training reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fbf69dc5b405>\u001b[0m in \u001b[0;36mload_clean_dataset\u001b[0;34m(vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load and clean a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/neg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f251ca0ac197>\u001b[0m in \u001b[0;36mprocess_docs\u001b[0;34m(directory, vocab)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_to_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9356f9d6f5e5>\u001b[0m in \u001b[0;36mdoc_to_line\u001b[0;34m(filename, vocab)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# filter by vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tokens' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab1.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load all training reviews\n",
    "docs,labels = load_clean_dataset(vocab)\n",
    "print(len(docs),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tokens' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-93b4b616611b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# load all training reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93b4b616611b>\u001b[0m in \u001b[0;36mload_clean_dataset\u001b[0;34m(vocab)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# load and clean a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/neg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93b4b616611b>\u001b[0m in \u001b[0;36mprocess_docs\u001b[0;34m(directory, vocab)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_to_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93b4b616611b>\u001b[0m in \u001b[0;36mdoc_to_line\u001b[0;34m(filename, vocab)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# filter by vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tokens' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    text = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory,vocab):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path,vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab)\n",
    "    docs = neg + pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs,labels\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab1.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load all training reviews\n",
    "docs,labels = load_clean_dataset(vocab)\n",
    "print(len(docs),len(labels))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 25768) (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r' )\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words( 'english' ))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' ' .join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "    \n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg' , vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos' , vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab1.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 First Sentiment Analysis Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.6912 - accuracy: 0.6089\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.6800 - accuracy: 0.8928\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.6607 - accuracy: 0.8667\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.6341 - accuracy: 0.8656\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.5996 - accuracy: 0.9278\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.5592 - accuracy: 0.9317\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.5178 - accuracy: 0.9422\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.4739 - accuracy: 0.9550\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.4318 - accuracy: 0.9650\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.3917 - accuracy: 0.9644\n",
      "Test Accuracy: 87.000000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r' )\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words( 'english' ))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' ' .join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "    \n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg' , vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos' , vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model,to_file='model.png',show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab1.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "# print(Xtrain.shape, Xtest.shape)\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "model.fit(np.array(Xtrain),np.array(ytrain),epochs=10,verbose=2)\n",
    "loss,acc = model.evaluate(np.array(Xtest),np.array(ytest),verbose=0)\n",
    "print('Test Accuracy: %f' %(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Word Scoring Methods\n",
    "The **texts_to_matrix()** function for the **Tokenizer** in the keras API provides 4 different methods for scoring words : \n",
    "* **binary** : Where words are marked as present(1) or absent(0).\n",
    "* **count** : Where the occurence count for each word is marked as integer.\n",
    "* **tfidf** : Where each word is scored based on the frequency,where words that are common across all documents are penalized.\n",
    "* **freq** : Where words are scored based on their frequency of occurence within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare bag-of-words encoding of docs\n",
    "def prepare_data(train_docs,test_docs,mode):\n",
    "    # create the tokenizer\n",
    "    tokenzer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs,mode=mode)\n",
    "    # encode testing data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs,mode=mode)\n",
    "    return Xtrain,Xtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain,ytrain,Xtest,ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 30\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        # compile network\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        # fit network\n",
    "        model.fit(Xtrain,ytrain,epochs=10,verbose=2)\n",
    "        # evaluate\n",
    "        loss,acc = model.evaluate(Xtest,ytest,verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' %((i+1),acc))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4725 - accuracy: 0.7861\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0595 - accuracy: 0.9933\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "1 accuracy: 0.925000011920929\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4653 - accuracy: 0.7961\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0606 - accuracy: 0.9972\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.1305e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.2999e-04 - accuracy: 1.0000\n",
      "2 accuracy: 0.9399999976158142\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4734 - accuracy: 0.7672\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0650 - accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 6.9919e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 4.5883e-04 - accuracy: 1.0000\n",
      "3 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4712 - accuracy: 0.7878\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0612 - accuracy: 0.9922\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.0438e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.2303e-04 - accuracy: 1.0000\n",
      "4 accuracy: 0.9200000166893005\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4750 - accuracy: 0.7833\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0649 - accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "5 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4667 - accuracy: 0.7839\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0541 - accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.9751e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 7.0139e-04 - accuracy: 1.0000\n",
      "6 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4607 - accuracy: 0.7861\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0459 - accuracy: 0.9967\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.2753e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.2631e-04 - accuracy: 1.0000\n",
      "7 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4674 - accuracy: 0.7817\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0536 - accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 9.6518e-04 - accuracy: 1.0000\n",
      "8 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4910 - accuracy: 0.7783\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0683 - accuracy: 0.9917\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.2480e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.3027e-04 - accuracy: 1.0000\n",
      "9 accuracy: 0.925000011920929\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4541 - accuracy: 0.7944\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0406 - accuracy: 0.9983\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 9.9835e-04 - accuracy: 1.0000\n",
      "10 accuracy: 0.9200000166893005\n",
      "Epoch 1/10\n",
      "57/57 - 2s - loss: 0.4856 - accuracy: 0.7761\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0690 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 9.8274e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 7.0335e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 5.3421e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 4.2201e-04 - accuracy: 1.0000\n",
      "11 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4674 - accuracy: 0.7844\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0560 - accuracy: 0.9928\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "12 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4602 - accuracy: 0.7978\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0482 - accuracy: 0.9967\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.3430e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.2328e-04 - accuracy: 1.0000\n",
      "13 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4575 - accuracy: 0.7989\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0491 - accuracy: 0.9972\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 8.4940e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 6.4238e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 5.0604e-04 - accuracy: 1.0000\n",
      "14 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4763 - accuracy: 0.7717\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0586 - accuracy: 0.9956\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 8.3657e-04 - accuracy: 1.0000\n",
      "15 accuracy: 0.9399999976158142\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4590 - accuracy: 0.7883\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0546 - accuracy: 0.9956\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 9.5805e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 7.4279e-04 - accuracy: 1.0000\n",
      "16 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4634 - accuracy: 0.7744\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0505 - accuracy: 0.9956\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.1188e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 5.7585e-04 - accuracy: 1.0000\n",
      "17 accuracy: 0.9200000166893005\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4785 - accuracy: 0.7644\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0605 - accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 9.8000e-04 - accuracy: 1.0000\n",
      "18 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4699 - accuracy: 0.7778\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0595 - accuracy: 0.9911\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.3219e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.2773e-04 - accuracy: 1.0000\n",
      "19 accuracy: 0.925000011920929\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4735 - accuracy: 0.7650\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0642 - accuracy: 0.9900\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.6210e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.4635e-04 - accuracy: 1.0000\n",
      "20 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4566 - accuracy: 0.7861\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0570 - accuracy: 0.9917\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 8.1475e-04 - accuracy: 1.0000\n",
      "21 accuracy: 0.925000011920929\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4691 - accuracy: 0.7894\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0578 - accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 8.6585e-04 - accuracy: 1.0000\n",
      "22 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4655 - accuracy: 0.7783\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0559 - accuracy: 0.9933\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "23 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4698 - accuracy: 0.7972\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0514 - accuracy: 0.9972\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 8.1224e-04 - accuracy: 1.0000\n",
      "24 accuracy: 0.9399999976158142\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4577 - accuracy: 0.7939\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0536 - accuracy: 0.9928\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 7.4570e-04 - accuracy: 1.0000\n",
      "25 accuracy: 0.9399999976158142\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4554 - accuracy: 0.7867\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0462 - accuracy: 0.9972\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 9.6003e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 7.4818e-04 - accuracy: 1.0000\n",
      "26 accuracy: 0.9350000023841858\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4832 - accuracy: 0.7956\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0691 - accuracy: 0.9928\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 8.4952e-04 - accuracy: 1.0000\n",
      "27 accuracy: 0.9449999928474426\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4689 - accuracy: 0.7944\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0612 - accuracy: 0.9967\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0047 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 9.6534e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 6.6851e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 4.8634e-04 - accuracy: 1.0000\n",
      "28 accuracy: 0.9300000071525574\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4804 - accuracy: 0.7828\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0677 - accuracy: 0.9933\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 7.5256e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 5.6013e-04 - accuracy: 1.0000\n",
      "29 accuracy: 0.9200000166893005\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4898 - accuracy: 0.7606\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0721 - accuracy: 0.9922\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.6184e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.7572e-04 - accuracy: 1.0000\n",
      "30 accuracy: 0.925000011920929\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4692 - accuracy: 0.7761\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0532 - accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.8253e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.6313e-04 - accuracy: 1.0000\n",
      "1 accuracy: 0.8949999809265137\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4522 - accuracy: 0.7917\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0567 - accuracy: 0.9922\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 8.3223e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.0956e-04 - accuracy: 1.0000\n",
      "2 accuracy: 0.8949999809265137\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4657 - accuracy: 0.7728\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0533 - accuracy: 0.9928\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 8.7610e-04 - accuracy: 1.0000\n",
      "3 accuracy: 0.9049999713897705\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4419 - accuracy: 0.7817\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0452 - accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0122 - accuracy: 0.9994\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 9.9843e-04 - accuracy: 1.0000\n",
      "4 accuracy: 0.8999999761581421\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4508 - accuracy: 0.7928\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.0557 - accuracy: 0.9872\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 7.7630e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 6.1295e-04 - accuracy: 1.0000\n",
      "5 accuracy: 0.8949999809265137\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.4459 - accuracy: 0.7894\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0ce395058a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# evaluate model on data for mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-255c97301676>\u001b[0m in \u001b[0;36mevaluate_mode\u001b[0;34m(Xtrain, ytrain, Xtest, ytest)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "# run experiment\n",
    "modes = ['binary','count','tfidf','freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    # prepare data for mode\n",
    "    Xtrain,Xtest = prepare_data(train_docs,test_docs,mode)\n",
    "    # evaluate model on data for mode\n",
    "    results[mode] = evaluate_mode(np.array(Xtrain),np.array(ytrain),np.array(Xtest),np.array(ytest))\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          binary\n",
      "count  30.000000\n",
      "mean    0.930833\n",
      "std     0.006706\n",
      "min     0.920000\n",
      "25%     0.925000\n",
      "50%     0.930000\n",
      "75%     0.935000\n",
      "max     0.945000\n"
     ]
    }
   ],
   "source": [
    "print(results.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicting Sentiment for New Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review,vocab,tokenizer,model):\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line],mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded,verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1 - percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test positive text\n",
    "text = 'Best Movie ever! It was great, I recommend it.'\n",
    "percent,sentiment = predict_sentiment(text,vocab,tokenizer,model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' %(text,sentiment,percent*100))\n",
    "\n",
    "# test negative text\n",
    "text = 'This is a bad movie'\n",
    "percent, sentiment = predict_sentiment(text,vocab,tokenizer,model)\n",
    "print('Review: [%s]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
